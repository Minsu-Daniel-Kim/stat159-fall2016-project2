# Predictive Modeling Process - Project Report


**Author**: Minsu Kim and Lingjie Qiao 

**Course**: Statistics 159

**Course Title**: Reproducible and Collaborative Statistical Data Science

**Instructor**: Gaston Sanchez

---

## Abstract

This paper summarizes the results of Stats 159 Reproducible and Collaborative Statistical Data Science Project 2: Predictive Modeling Process. After learning and applying multiple linear regression model via Ordinary Least Squares (OLS), we recognized some potential insufficiency of the ordinary least squares model and develop the idea of performing a predictive modeling process applied on the data set `Credit`. This project is largely based on the instructions provided in _Chapter 6: Linear Model Selection and Regularization_ from book **"An Introduction to Statistical Learning"**. 

The goal of this project is to present the use of predictive modeling process and utilize software tools that effectively communicate the results. We are therefore dedicated to maintain project reproducibility and provide both objective and personal reflections upon regression analysis.

---

## 1. Introduction

This project thoroughly explores the predictive modeling process. From previous study, in order to understand the relationship of one dependent variable with several independent variables, we fit a multiple linear regession with Ordinary Least Squares. However, since OLS may have high variance and include irrelevant variables, Predictive Modeling Process can improve the results in terms of **Prediction Accuracy** and **Model Interpretability**. 

According to the book "An Introduction to Statistical Learning", alternative fitting procedures can yield better results in the following perspectives:

* Prediction Accuracy: Provided that the true relationship between the response and the predictors is approximately linear, the least squares estimates will have low bias. If n ≫ p—that is, if n, the number of observations, is much larger than p, the number of variables—then the least squares estimates tend to also have low variance, and hence will perform well on test observations. However, if n is not much larger than p, then there can be a lot of variability in the least squares fit, resulting in overfitting and consequently poor predictions on future observations not used in model training. And if p > n, then there is no longer a unique least squares coefficient estimate: the variance is infinite so the method cannot be used at all. By constraining or shrinking the estimated coefficients, we can often substantially reduce the variance at the cost of a negligible increase in bias. This can lead to substantial improvements in the accuracy with which we can predict the response for observations not used in model training.
* Model Interpretability: It is often the case that some or many of the variables used in a multiple regression model are in fact not associated with the response. Including such irrelevant variables leads to unnecessary complexity in the resulting model. By removing these variables—that is, by setting the corresponding coefficient estimates to zero—we can obtain a model that is more easily interpreted. Now least squares is extremely unlikely to yield any coefficient estimates that are exactly zero. In this chapter, we see some approaches for automatically performing feature selection or variable selection—that is, for excluding irrelevant variables from a multiple regression model.

The following analysis therefore utilizes four different kinds of regression models to find the best fitting model for predictive modeling process.

---
## 2. Data

We download the data set `Credit` from online link `http://www-bcf.usc.edu/~gareth/ISL/Credit.csv`, which is provided by the author of the book, "An Introduction to Statistical Learning". This data set records `Balance`, which is the average credit card debt for a number of individuals, as well as several predictors. The dataset has eleven variables - seven **quantitative** variables, `Income`, `Limit`, `Rating`, `Cards`, `Age`, `Education`, and `Balance`, and four **qualitative** variables, `Gender`, `Student`, `Married`, and `Ethnicity`. Our goal is to understand the relationship between `Balance` and these potential predictors with statistical fitting procedures.

### 2.1 Pre-modeling Data Processing

In order to fit the regression models, we first preprocess the dataset `Credit` with two steps:
* convert factors into dummy variables - which avoids the problem of input data as factors
* mean centering and standardization - which provides comparable scales for data analysis

---
## 6. Conclusion

In conclusions, we explore and compare the usage of different regression models on dataset `Credit` to understand the relationship between dependent variable `Balance` and ten potential predictors. Setting ordinary least squares as the benchmark, we look at two shrinkage regression methods (ridge and lasso) and two dimension reduction regression methods (PCR and PLSR) to find the best fitting model.

## Reference

James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. _An Introduction to Statistical Learning: With Applications in R_. New York: Springer, 2013. Print.